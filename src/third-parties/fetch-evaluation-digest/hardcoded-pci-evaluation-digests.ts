export const hardcodedPciEvaluationDigests = {
  '10.24072/pci.neuro.100217.d1': `<div class="pci2-recomm-review-container" id="d-320">
      <h4 class="pci2-recomm-text-label">Decision by <span><span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=631"><b>Amanda Almacellas Barbanoj</b></a><a href="https://orcid.org/0000-0001-8816-2642" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0001-8816-2642"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span> and <span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=9"><b>Mahesh Karnani</b></a><a href="https://orcid.org/0000-0002-4810-4146" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0002-4810-4146"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span></span>, <span><i>posted 15 Oct 2024</i></span><span><i>, validated 15 Oct 2024</i></span></h4>
      <div class="pci2-recomm-text">
        <p>The authors present an open source statistical power calculator in binary choice tasks often used in behavioural assessment, and demonstrate its utility with an example behavioural experiment.&nbsp;</p>

<p>This preprint has been reviewed by four reviewers, who generally saw merit in the work, although some major concerns were raised by Dr Lakens and anonymous reviewer 1, that we think should be addressed in detail. Particularly, the nested structure of the data. Addressing this would be a useful service to the field, as results are sometimes tested with the assumption that repeated measurements from the same animal are independent. In addition to Dr Lakens’ references, another useful reference is <a href="https://doi.org/10.1038/nn.3648">https://doi.org/10.1038/nn.3648</a> . We feel that ideally the authors would modify their simulation code to take this into account, and perhaps make it available as an option in the GUI, such that future users can see for themselves the difference. Additionally, a figure comparing results with and without this option might be useful.&nbsp;</p>

<p><br>In addition to the reviewer’s comments, the recommenders noted additional minor points:</p>

<p>In order to understand how different the 2-arm and 4-arm situations were, the behavioural experiment would benefit from a more detailed description:</p>

<ul>
<li>Were groups 1 and 2 the same 6 mice? If not, please clarify in text under ‘Animals’ because it implies that this was the case. If yes, which was conducted first?&nbsp;</li>
<li>Please clarify definitions of session and block - if they are the same, please only use one word.</li>
<li>Was limonene the only odour?</li>
<li>What is classified as an animal visiting an arm? (reaching the reward hole, X% of the body entering the arm, or other criterion?)</li>
<li>How deep are the reward holes?</li>
<li>How big is the reward?</li>
<li>How were animals returned to the start box?</li>
<li>How were the reward holes reset? Were they cleaned? How was randomization of the correct arm sequence determined by the investigator?</li>
<li>Authors mention the NC3Rs, which also offer power analysis tools for optimising experimental design e.g., at https://eda.nc3rs.org.uk/. It may be worth noting how the authors’ tool compares to this.</li>
<li>Figures need attention:<br>Fig2, chance ½ and ¼ wrong way around, panel labels in very small font<br>Fig3, x-ticks not aligned with labels or data, spelling of ‘success’, panel labels very small.<br>Fig4, too many font sizes, clarify ‘8 trials’ refers only to block 16 e.g. with an arrow.&nbsp;</li>
</ul>


      
      </div>
    </div>`,
  '10.24072/pci.neuro.100217.rev11': `<div id="review-513" class="review-container">
      <h4 class="pci2-round-review-sub-title"><span id="review-513">Reviewed by <a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=783"><b>Georgie Mills</b></a>, 03 Oct 2024</span></h4>

      <div style="border-left: 1px solid #eee; padding: 5px 15px; margin-bottom: 25px;">
        
        <p>This preprint aims to address the specialised aspects of behavioural study designs that frequently lead to undermining reproducibility of results due to low statistical power. The authors aim to address an ongoing issue with behavioural studies and provide appropriate context to the issue being addressed.&nbsp;</p><p>The title of the paper clearly reflects the contents throughout. Hypotheses are well presented and referred back to throughout the results and discussion. The article is well constructed, providing a detailed overview of the statistical package and applying experimental data from a commonly used behaviour to test the ability of the package to do as they deem required. The authors clearly present the rationale for designing this package and motivation as to why this is required in the field of behavioural neuroscience.The methodology is properly employed and well explained. The conclusions are well supported by the results given.</p><p>The authors provide a detailed comparison of current methods of statistical testing and their designed package coupled with an excellent description of how the different methods are utilised to bridge the gap the article is addressing. This is clear, concise but detailed in an understandable way. This enables users to easily utilise this within their study design in any lab.&nbsp;</p><p>The results clearly show how the statistical package has been generated that can enable small sample sizes to gain high statistical power. Mainly through the use of a monte-carlo simulation. Application of this method on experimental data concluded that reducing chance level enabled significance to be reached, at a level where 5xs more animals would normally be required. Highlighting the advantage of using this statistical package from an ethical viewpoint, thereby addressing the question if using less animals can impact the quality outcomes of statistics performed on the test. This is an aspect I congratulate the authors on highlighting and aiming to overcome. The consideration of different parameters that can affect significance is a strong point of this preprint. And importantly, bringing to question how experimental design can impact the outcomes of a study in a niche where these issues are prevalent.&nbsp;</p><p>The queries I had with the approach and design are well discussed within the limitations of the study, making this well-balanced in the discussion. For example, the issue of using the same number of trials for each animal tested within the study. An aspect which could cause performance differences in each subject due to testing some animals more than others. I would also query how much should be changed in terms of experimental design to gain statistical significance, I would like to hear from the authors in the discussion on what is the limit to manipulate the data in terms of experimental design using this statistical package.&nbsp;</p><p>Minor corrections: For figure 2, labels are opposing each other from in-text and in-figure details. They currently show 2 arms with chance ¼ and group 4 arms of chance ½ - this is opposite as to what is in the text.</p><p>I would be interested to see how this methodology impacts other behavioural paradigms widely used in neuroscience where fatigue plays a large factor when increasing trial number. Or, a reward-independent paradigm, where sometimes variability is higher between mice. As currently, I would infer this method is only successful for associative learning paradigms. Hence I think comparison to another behaviour would support this notion to a higher degree.&nbsp;</p><p>It would also be interesting to understand within the preprint how varying the monte-carlo simulation number of iterations would impact the outcome due to noting “a higher number provides greater precision in the power estimation.”&nbsp;</p><p>But overall, a really nice approach to bridging the gap in low sample sizes and the issue of statistical significance in behavioural neuroscience where variability in cognitive ability is present. A great benefit to current aspects of behavioural studies.&nbsp;I congratulate the authors in providing a means to optimise the experimental design for behavioural paradigms for associative learning&nbsp;</p><p style="display: none;">&nbsp;</p><p style="display: none;">&nbsp;</p>

        

        
        <a class="doi-url-subsection" href="https://doi.org/10.24072/pci.neuro.100217.rev11" target="_blank">https://doi.org/10.24072/pci.neuro.100217.rev11</a>
      </div>
    </div>`,
  '10.24072/pci.neuro.100217.rev12': `<div id="review-516" class="review-container">
      <h4 class="pci2-round-review-sub-title"><span id="review-516">Reviewed by <span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=786"><b>Daniel Lakens</b></a><a href="https://orcid.org/0000-0002-0247-239X" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0002-0247-239X"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span>, 08 Oct 2024</span></h4>

      <div style="border-left: 1px solid #eee; padding: 5px 15px; margin-bottom: 25px;">
        
        <p>This paper is written around the code for a power simulation for a single specific study. It is relatively uncommon to publish code for a specific power simulation as a separate article. A problem is that the power analysis as it is simulated is fundamentally flawed, as the authors ignore the multi-level (or hierarchical) structure in the data, and treat the repeated trials from the same animals as independent data (as far as I could tell from the code). This is not correct.&nbsp;</p><p style="display: none;">&nbsp;</p><p>1. The authors should simulate a multi-level test (i.e., multi-level logistic regression)</p><p style="display: none;">&nbsp;</p><p>The reason for this is that the moment the same individual performs multiple trials, we have a multi-level structure – trials are nested inside mice, and mice are nested in condition. We can not treat all trials from the mice as independent from each other – they are not all independent observations – some come from the same mice.&nbsp;</p><p style="display: none;">&nbsp;</p><p>The basic idea of such power analyses is nice explained in: Westfall, J., Kenny, D. A., &amp; Judd, C. M. (2014). Statistical power and optimal design in experiments in which samples of participants respond to samples of stimuli. Journal of Experimental Psychology: General, 143(5), 2020.</p><p style="display: none;">&nbsp;</p><p>The authors are performing power analyses for a multilevel logistic regression, if I understood their goals well. This is explained in the following paper:&nbsp;</p><p>Olvera Astivia, O. L., Gadermann, A., &amp; Guhn, M. (2019). The relationship between statistical power and predictor distribution in multilevel logistic regression: A simulation-based approach. BMC Medical Research Methodology, 19(1), 97. https://doi.org/10.1186/s12874-019-0742-8</p><p>There is also an associated shiny app to perform these calculations.&nbsp;</p><p>It was difficult to clearly see what the authors intended to do in their power analysis. There is no math, not clear discussion of the tests they are simulating. If I look at their code, line 79 says:&nbsp;</p><p style="display: none;">&nbsp;</p><p>p=scipy.stats.ttest_ind(animals_sucess1,animals_sucess2,alternative=alter)[1]”</p><p style="display: none;">&nbsp;</p><p>I am not familiar with statistical tests in python (I use R) but it seems the authors are just running a t-test. This is clearly wrong. It would probably be a good idea to invite a statistician to collaborate on this project.&nbsp;</p><p style="display: none;">&nbsp;</p><p>I might have gotten this wrong – if so, my apologies, but then my review at least explains that it is not clear to me which power analysis is performed. The authors should provide much more detail.&nbsp;</p><p style="display: none;">&nbsp;</p><p>2. There are no validation documents</p><p style="display: none;">&nbsp;</p><p>Typically when authors want to publish software for power analysis, I would expect to see extensive validation documents. This consists of an overview of simulations that show expected results - for example a nominal Type 1 error rate, and the same results as other power analysis tools.&nbsp;</p><p style="display: none;">&nbsp;</p><p>For example, when there are no effects, we should observe nominal alpha levels. So, if a 5% alpha is used, 5% of the tests are significant.&nbsp;</p><p style="display: none;">&nbsp;</p><p>When I ran the software with no differences between groups, and equal groups, 1 trial, 1 repetition, and 100.000 trials the Type 1 error rate was 6.14. It always was a bit high when I tested it. Of course, it should be 5%. If it is too high, this might be a problem with the code. It is difficult to say, but I personally would not trust the code.&nbsp;</p><p style="display: none;">&nbsp;</p><p>If you search the literature, I believe there is other software out there that can perform at least some of the same tests. The validation document should then demonstrate that the same results are observed. In this case, as the authors do not actually simulate a multi-level data structure and perform a multi-level test, so their results will not align, but this would have provided them an indication that they are doing something wrong. Performing these simulations is difficult, as you need the within and between cluster correlations, which are very difficult to know (or to estimate with the small sample sizes used in animal research). Nevertheless, these are essential for valid simulations.</p><p style="display: none;">&nbsp;</p><p>3. Discussion of a real study</p><p style="display: none;">&nbsp;</p><p>The article also presents information about the single mouse study the analysis was created for, but all empirical sections are superfluous. There is no added value in discussing this study, and the methods – it is completely irrelevant for the power analysis tool. These sections should not be part of a paper on power analysis.&nbsp;</p><p>Although I think the empirical sections should be removed, there was no link in the sentence: "Both the behavioral protocol and analytical plan were pre-registered (here, OSF)."</p><p style="display: none;">&nbsp;</p><p>4. Minor:&nbsp;</p><p>When I run the code, a warning appearer that worry me. Maybe they should not worry me, but, perhaps, these are problems – regardless, it is not nice to let users worry about warnings. Either prevent them, or explain why they occur and why I do not need to worry. I got another warning one time but I did not store it. Perhaps test the code a bit more.&nbsp;</p><p>/srv/conda/envs/notebook/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.</p><p>&nbsp; return _methods._mean(a, axis=axis, dtype=dtype,</p><p>/srv/conda/envs/notebook/lib/python3.12/site-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide</p><p>&nbsp; ret = ret.dtype.type(ret / rcount)</p><p style="display: none;">&nbsp;</p><p>I saw the following spelling error in the tool : ### **SuccesRatePower**: statistical power calculator for bheavioural neuroscience</p><p style="display: none;">&nbsp;</p><p>There was also a spelling error: oeverlook</p><p>The authors write: "These results highlight a significant advantage in reducing the number of subjects required by adopting an experimental procedure with a low chance level." Of course if we use the largest manipulation we can think of, the effect size is large, and therefore power is higher, all else equal. This is a trivial fact mentioned in many power analysis papers. But, typically we do not choose an effect size to study - we choose a research question, which has a certain effect size. So, we should not choose large effects to study, but study interesting questions.&nbsp;</p><p style="display: none;">&nbsp;</p><p>The authors use Bayes factors to support the null, but do not discuss their priors. This is extremely bad statistical practice. Either use meaningful priors, or use equivalence testing instead for null effects.&nbsp;</p><p style="display: none;">&nbsp;</p><p>Signed,</p><p style="display: none;">&nbsp;</p><p>Daniel Lakens</p>

        

        
        <a class="doi-url-subsection" href="https://doi.org/10.24072/pci.neuro.100217.rev12" target="_blank">https://doi.org/10.24072/pci.neuro.100217.rev12</a>
      </div>
    </div>`,
  '10.24072/pci.neuro.100217.rev13': `<div id="review-519" class="review-container">
      <h4 class="pci2-round-review-sub-title"><span id="review-519">Reviewed by anonymous reviewer 1, 29 Sep 2024</span></h4>

      <div style="border-left: 1px solid #eee; padding: 5px 15px; margin-bottom: 25px;">
        
        <p style="display: none;"></p><p>Desachy et al. (2024) describe using a Monte Carlo simulation to calculate statistical power for experiments that include a dichotomous variable (success/failure). They find that statistical power is increased where the probability of succeeding by chance is lower, more trials are used and accompanied by appropriate statistical analyses. They also ran an associative behavioural experiment they argue shows empirical support for their statistical simulations.</p><p>I have briefly reviewed the python script and verified that Jupyter notebook allows for running the script in a web browser.</p><p>In my view, this paper provides a useful power calculator for several kinds of associative learning experiments.</p><p><strong>PCI Questionnaire</strong></p><p><strong>Title and abstract</strong></p><ul><li>Does the title clearly reflect the content of the article? <em>Yes</em></li><li>Does the abstract present the main findings of the study? <em>Yes</em></li></ul><p><strong>Introduction</strong></p><ul><li>Are the research questions/hypotheses/predictions clearly presented? <em>Yes</em></li><li>Does the introduction build on relevant research in the field? <em>Yes</em></li></ul><p><strong>Materials and methods</strong></p><ul><li>Are the methods and analyses sufficiently detailed to allow replication by other researchers? <em>Yes</em></li><li>Are the methods and statistical analyses appropriate and well described? <em>Yes - script is open source, MIT licensed and runs as a Jupyter notebook. Generating data from a binomial function is sensible. Most formulae look correct to me.</em></li></ul><p><strong>Results</strong></p><ul><li>In the case of negative results, is there a statistical power analysis (or an adequate Bayesian analysis or equivalence testing)? <em>Yes - Power analysis is the topic of the paper.</em></li><li>Are the results described and interpreted correctly? <em>Yes</em></li><li>Do the results support the conclusions? <em>Yes</em></li><li>Are the results justified by the methods? <em>Yes</em></li></ul><p><strong>Discussion</strong></p><ul><li>Have the authors appropriately emphasized the strengths and limitations of their study/theory/methods/argument? <em>Yes (but see below for some comments)</em></li><li>Are the conclusions adequately supported by the results (without overstating the implications of the findings)? <em>Yes, I particularly like the noting of the ethical imperative to reduce animal use by appropriately powering our experiments.</em></li></ul><p><strong>Specific</strong> <strong>Comments</strong></p><ol><li>My main concern is that the power calculator doesn't let users calculate a target N (unlike G*Power). I understand that this is quite difficult and would essentially need an algorithm to iterate and run thousands of simulations for each iteration. Some discussion for how to determine N for a study using this tool may be warranted.</li><li>The inclusion of the animal experiment is a good attempt to tie the theoretical statistical simulations to actual data and its 'real-world' experimental applications. However, I think it needs some more discussion of its limitations. This experiment can only validate some of the simplest and, perhaps obvious, inferences based on the simulations, such as increasing the number of trials. It doesn't actually validate that the statistical power calculations coming out of the simulations are correct (which might be assumed by a careless reader). I think that's an important distinction, even though it's impossible for the authors to actually run 5 or 10 thousand iterations of their study.<ul><li>Another way the authors could approach this is to run a meta-analysis on data from prior studies. This would be labour-intensive and I don't want to require new experiments/analyses, but a lab with sufficient history will likely have run experiments with various designs that can be thrown together in analyses to see if they track according to the results of the simulation model.</li></ul></li><li>Technical comment: Is the statistical power formula on p. 6 line 144 correct? The false negative rate looks more like Beta to me, whereas statistical power is 1-Beta.</li><li>Technical comment: I'm a little confused by the use of Bayesian statistics in the results (p. 12, ~ line 288). In my thinking, statistical simulation studies are intrinsically frequentist, since they deal with long-run probabilities. Why was this approach used? I think this needs further explanation and justification. I have seen researchers criticised for reporting a BF01 to accept a null hypothesis before, but can't remember the exact justification, so if proceeding with a BF01 I might cite Keysers et al. (2020; doi:10.1038/s41593-020-0660-4) or another paper covering the topic of BF01 in behavioural neuroscience (Disclosure: I am not an author of Keysers et al.). If the intention is to verify whether the null hypothesis should be accepted, I would suggest that a frequentist equivalence test might be more appropriate (e.g., TOST) or that both frequentist equivalence and inverse Bayes can be reported.</li><li>Technical comment: Why were one-tailed t-tests used? Can't differences be both increases or decreases, justifying a use of a two-tailed test?</li></ol><p style="display: none;"></p>

        

        
        <a class="doi-url-subsection" href="https://doi.org/10.24072/pci.neuro.100217.rev13" target="_blank">https://doi.org/10.24072/pci.neuro.100217.rev13</a>
      </div>
    </div>`,
  '10.24072/pci.neuro.100217.rev14': `<div id="review-524" class="review-container">
      <h4 class="pci2-round-review-sub-title"><span id="review-524">Reviewed by <span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=794"><b>James McCutcheon</b></a><a href="https://orcid.org/0000-0002-6431-694X" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0002-6431-694X"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span>, 10 Oct 2024</span></h4>

      <div style="border-left: 1px solid #eee; padding: 5px 15px; margin-bottom: 25px;">
        
        <p>This short manuscript presents a new tool that can be used by researchers to increase the statistical power of behavioural neuroscience experiments by tweaking their design and analysis. Specifically, they focus on experiments in which sibjects are required to make a correct choice when offered multiple options (i.e., "success rates"). Statistical analysis generally involves comparing the number of successful choices observed to the number of that outcome expected by chance. The authors argue that three changes to methodology and analysis can result in dramatically increased statistical power. These changes are: (1) increasing the number of choices available per trial; (2) increasing the number of trials; and (3) using discrete value statistics with trial as unit of statistic instead of subject. The approach seems sound, and the authors perfomed an experiment with rats to show its utility and demonstrate the effect of modifying these parameters.&nbsp;</p><p>The authors provide a software tool in which users can simulate different experimental conditions and test the effect on achieved power. This allows for the method to be efficiently employed by those who can make use of it. The tool was easy to use and well packaged as a Binder notebook with underlying code available on Github. This approach can help researchers to reduce numbers of subjects which has positive ethical and financial implications.</p><p>If the authors were revising the manuscript, I would recommend expanding the discussion. In particular, I would like the authors to comment on whether some of these tweaks to methods and analysis could be used to increase power in other commonly used behavioural (or other) tasks. Low-powered studies are endemic throughout neuroscience and so initiatives such as this are very welcome.</p><p>In addition, there are some formatting changes I would recommend.</p><p>The arrangement of panels in figure 3 could be changed to enhance the flow with the labels (A, B, C etc) made larger. The description in the text (lines 236-237 especially) is a little confusing and could be sharpened up.</p><p>Input parameters in Methods might be better in a table.</p><p>The Methods section on Animals should refer to the fact that at some point in the study the animals will undergo mild food restriction.</p><p>Functions mentioned should be formatted consistenty, maybe with italics or a different font.</p><p>I did not see references to the supplemental figues in the text.</p><p>Line spacing is not consistent in paragraph on Animals in Methods.</p><p style="display: none;">&nbsp;</p>

        

        
        <a class="doi-url-subsection" href="https://doi.org/10.24072/pci.neuro.100217.rev14" target="_blank">https://doi.org/10.24072/pci.neuro.100217.rev14</a>
      </div>
    </div>`,
  '10.24072/pci.neuro.100217.d2': `<div class="pci2-recomm-review-container" id="d-321">
      <h4 class="pci2-recomm-text-label">Decision by <span><span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=631"><b>Amanda Almacellas Barbanoj</b></a><a href="https://orcid.org/0000-0001-8816-2642" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0001-8816-2642"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span> and <span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=9"><b>Mahesh Karnani</b></a><a href="https://orcid.org/0000-0002-4810-4146" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0002-4810-4146"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span></span>, <span><i>posted 21 Feb 2025</i></span><span><i>, validated 24 Feb 2025</i></span></h4>
      <div class="pci2-recomm-text">
        <p><br>Dear Nicola,</p>

<p><br>Many thanks for your revision. We appreciate that a multilevel mixed effects model was included to analyse effects within and between animals performing multiple trials. However, the key reviewer notes that this does not fully solve the issue of the variance structure that leads to correlations between observations of multiple trials from each animal. In addition, they suggest to amend the simulated data, by adding in dependencies between trials from individual animals, i.e., trials by the same animal should be more correlated than trials across all animals. We agree with their logic that simulating such realistic data would more accurately demonstrate how users should expect experimental parameters to affect statistical power. They also very constructively suggest discussing the variance-covariance matrix underlying the nested data-structure. We agree that this would be a constructive way forward. As we believe the manuscript may be misleading in its current state, we cannot currently recommend it, but welcome a revision. We apologise for the lengthy process on this occasion, and hope that you will continue developing this promising tool.</p>

<p><br>Additionally, we noted a likely typo with dimensions of the reward and the holes in the maze, did you mean ‘mm’ instead of ‘cm’ as the unit?&nbsp;</p>

<p style="display: none;">&nbsp;</p>

<p>Yours,</p>

<p>Mahesh and Amanda</p>

<p style="display: none;">&nbsp;</p>


        
      </div>
    </div>`,
  '10.24072/pci.neuro.100217.rev21': `<div id="review-525" class="review-container">
      <h4 class="pci2-round-review-sub-title"><span id="review-525">Reviewed by <span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=786"><b>Daniel Lakens</b></a><a href="https://orcid.org/0000-0002-0247-239X" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0002-0247-239X"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span>, 13 Feb 2025</span></h4>

      <div style="border-left: 1px solid #eee; padding: 5px 15px; margin-bottom: 25px;">
        
        <p>The authors have revised their paper, addressing several of my minor concerns. But my main concern about the power analysis remains, in that the authors are simulating multiple trials, but do not take into account the variance structure that leads to correlations between these observations. In the current simulation – unless I misunderstood what the authors are doing – each simulation is treated as an independent observation. In other words, whether we double the number of animals, or we double the number of trials, makes no difference – both lead to the same increase in power. But this is not correct, as all the references I send along in my previous email point out.&nbsp;</p><p>The authors have added the option to *analyze* the data with a mixed model, which is good, but my comment was about how they should *simulate* data that takes the nested structure of the data into account. They ignored this, and it remains as large a problem as I indicated in my previous review.&nbsp;</p><p>The authors say they are simulating data that has the characteristics of the target population, but this is not true, because they are not simulating data appropriately. For an example on how to simulate data for mixed designs (trials nested in animals) correctly, see https://debruine.github.io/faux/articles/sim_mixed.html. The Faux package in R allows you to simulate such data – I do not know of packages in python that allow you to do this.</p><p>To constructively move this review process forward, the authors need to demonstrate an understanding of the multi-level nature of their design, and discuss these aspects in their data. The authors need to discuss the variance-covariance matrix underlying their nested data-structure. Without discussing this, their simulation code will not be correct. As I mentioned before, the authors might want to include a statistician with expertise in this topic to their paper, it might help to efficiently incorporate my suggestions.&nbsp;</p><p>The point is that the multi-level model will start to show different – and correct – results, depending on the variance-covariance matrix entered. Importantly, I am pretty sure power will be much lower than the tool currently suggests, when the dependencies among trials from the same animals is taken into account appropriately. Currently, the authors just conclude mixed-models gives slightly more conservative results – but this will change if data is simulated correctly. That was the whole point of my original review. If this mistake it not corrected, anyone using this tool will believe they have higher power than they have, and studies will be badly designed.&nbsp;</p><p style="display: none;">&nbsp;</p><p>I also have a number of minor points. After the main point above is addressed, the points below also deserve attention.&nbsp;</p><p style="display: none;">&nbsp;</p><p>The authors write “The repeatability of the Monte Carlo simulation was assessed by repeating the power calculation 100 times with the same parameters and evaluating the standard deviation of the calculated power as a function of the number of iterations. As shown in Supplementary Figure 2, a variability lower than 1% was observed starting from 4,000 iterations.”&nbsp;</p><p>I am not sure why this is reported. Of course if you increase the number of simulations, variability becomes smaller. You do not need to illustrate this, it logically follows from the law of large numbers. So what is the goal of this addition?&nbsp;</p><p style="display: none;">&nbsp;</p><p>I appreciate the additional validation against G*Power. I just wanted to check there is really no other software, not even PASS, that allows any of the other computations to be performed? Validation is really important to check the quality of new code.&nbsp;</p><p style="display: none;">&nbsp;</p><p>The authors have added “Sample size was determine by an open-ended Sequential Bayes Factor desig starting with 6 mice per group and add new subject &nbsp;until BF01&lt;1/3 or BF01&gt;3.”</p><p style="display: none;">&nbsp;</p><p>Besides the typo in ‘desig’ this way to design a study is bad practice. As indicated in the paper by Schonbrodt and Wagenmakers on sequential Bayes factor designs, a threshold of BF &gt; 3 leads to very high error rates, which is why they recommend a minimal BF &gt; 6. So why did the authors design a study with high error rates? Is the point of the paper that their original approach was incorrect, and therefore people should use proper power analysis procedures as they describe in their paper?&nbsp;</p><p style="display: none;">&nbsp;</p><p style="display: none;">&nbsp;</p>

        

        
        <a class="doi-url-subsection" href="https://doi.org/10.24072/pci.neuro.100217.rev21" target="_blank">https://doi.org/10.24072/pci.neuro.100217.rev21</a>
      </div>
    </div>`,
  '10.24072/pci.neuro.100217.d3': `<div class="pci2-recomm-review-container" id="d-322">
      <h4 class="pci2-recomm-text-label">Decision by <span><span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=631"><b>Amanda Almacellas Barbanoj</b></a><a href="https://orcid.org/0000-0001-8816-2642" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0001-8816-2642"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span> and <span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=9"><b>Mahesh Karnani</b></a><a href="https://orcid.org/0000-0002-4810-4146" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0002-4810-4146"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span></span>, <span><i>posted 13 Mar 2025</i></span><span><i>, validated 14 Mar 2025</i></span></h4>
      <div class="pci2-recomm-text">
        <p>Dear Nicola,</p>

<p>Thank you for providing a revised version. We wonder if you sent the correct tracked-changes file, as it has everything from line 261 labelled as changed, and the figure 2 legend still mentions 31cm diameter reward holes. Please could you send us the correct tracked-changes file or a list of the changes you have made?</p>

<p>In addition, could you clarify what you mean by ‘correlation of within-trial outcomes’ in your response. As far as we understand, there is only one outcome within a trial, and possibly another term was meant here?&nbsp;</p>

<p>The reason we are asking for these clarifications before moving forward is that we need to understand why the main point raised by the reviewer regarding the way the data is simulated has not been completely addressed. We appreciate that the success rate for each subject determines the within-subject trial success rate, leading to not-independent trials for each subject within an experimental group. Still, the fact that increasing the number of trials increases the power of the analysis needs to be explained. As we understand it, the experimental units in an experiment comparing the performance of subjects during a learning task, are the subjects, not the trials. Therefore, since the number of trials doesn’t affect the success rate of the animals (as stated in your manuscript -lines349-352-), the parameter should not affect the power of the experimental design. We note that the key reviewer also mentioned that “...whether we double the number of animals, or we double the number of trials, makes no difference – both lead to the same increase in power. But this is not correct,...”, which should be addressed directly.</p>

<p>Yours,</p>

<p style="display: none;">&nbsp;</p>

<p>Amanda and Mahesh</p>

<p style="display: none;">&nbsp;</p>


        
      </div>
    </div>`,
  '10.24072/pci.neuro.100217.d4': `<div class="pci2-recomm-review-container" id="d-326">
      <h4 class="pci2-recomm-text-label">Decision by <span><span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=631"><b>Amanda Almacellas Barbanoj</b></a><a href="https://orcid.org/0000-0001-8816-2642" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0001-8816-2642"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span> and <span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=9"><b>Mahesh Karnani</b></a><a href="https://orcid.org/0000-0002-4810-4146" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0002-4810-4146"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span></span>, <span><i>posted 07 Aug 2025</i></span><span><i>, validated 08 Aug 2025</i></span></h4>
      <div class="pci2-recomm-text">
        <p>Dear Nicola,</p>

<p>Thank you for your revised submission. We and the key reviewer agree that it now appropriately handles the covariance structure. Therefore, we are very close to recommending the manuscript. However, some minor inconsistencies and one major one has been introduced into the latest version which need to be explained or revised:</p>

<p>1) Currently two figures are labelled as Fig2.</p>

<p>2) *Major issue. The first Fig2 shows the relevant scenario of control performing well and treated cohorts possibly performing less well. However, Figs 3B and 4A have control cohorts performing at chance or slightly above. No justification is given for such a scenario (is this some form of negative control, or naive condition pre-training, or a gain-of-function experiment?). The relevant scenario (loss-of-function) is justified well in the text describing first fig2. I think this is the typical use case, and Figs 3 and 4 should also have controls performing at a high success rate. I also note that the tool does not work if group one is performing lower than group two.</p>

<p>3) This version was very difficult to read:<br>-Many grammatical errors and typos that should be caught by a spell-check software such as libreoffice.</p>

<p>-Inconsistencies like ‘intrasubject’ in text (line 389) and ‘intersubject’ in the corresponding figure, and inconsistent sentence on lines 348-350: was ‘success rate’ rather than ‘chance level’ meant on line 350?</p>

<p>-No paragraph breaks in first results section.</p>

<p>-Text on the top of the page is frequently under the biorxiv header: please increase top margin size.</p>

<p>→Could authors please proof-read, and spell-check with the intention that this is aiming at a final version.</p>

<p>4) X-axes with chance level are inverted and not linear -&gt; please either relabel to ‘number of alternative choices’ or flip axis and linearize intervals between ticks.</p>

<p>We trust these would not take a long time to clear out, but do let us know if you require more than one month.</p>

<p><br>Best wishes,<br>Mahesh Karnani</p>


        
      </div>
    </div>`,
  '10.24072/pci.neuro.100217.rev41': `<div id="review-549" class="review-container">
      <h4 class="pci2-round-review-sub-title"><span id="review-549">Reviewed by <span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=786"><b>Daniel Lakens</b></a><a href="https://orcid.org/0000-0002-0247-239X" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0002-0247-239X"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span>, 17 Jul 2025</span></h4>

      <div style="border-left: 1px solid #eee; padding: 5px 15px; margin-bottom: 25px;">
        
        <p>I guess the authors never have to question the usefulness of peer review in the remainder of their career. Imagine if I had not been a reviewer, they would have published the original version of their simulation tool, and it had become popular! It would have been extremely problematic.&nbsp;</p><p>I’m happy to see that after multiple reviews the authors finally got my main criticism. As a consequence, we are looking at a completely new version of the manuscript and the tool.&nbsp;</p><p>I did not see any remaining issues with the manuscript – but I also did not run the tool myself. And, I might be not the sharpest after seeing multiple versions, and believing I have spent more time on this review process than can reasonably be asked of a single person. An new reviewer might not be a bad idea, but then the review process has already taken so long. IN any case, I carefully read the text, but nothing jumped at me as clearly wrong.&nbsp;</p><p style="display: none;">&nbsp;</p><p style="display: none;">&nbsp;</p><p>Minor points:&nbsp;</p><p>If I would create software, I would always include some validation files, where I compare results against other tools – even if only for much simpler versions of a test that are available in other software, such as single trials per animal. It is a recommendation, as it can help t identify any subtle changes in calculations between tools that might be informative.&nbsp;</p><p style="display: none;">&nbsp;</p><p>positif &nbsp; &gt; positive</p><p>positifs (formula page 7 – positives</p><p>Scripts &nbsp;is &gt; Scripts are</p><p style="display: none;">&nbsp;</p><p>Signed,&nbsp;</p><p style="display: none;">&nbsp;</p><p>Daniel Lakens</p>

        

        
        <a class="doi-url-subsection" href="https://doi.org/10.24072/pci.neuro.100217.rev41" target="_blank">https://doi.org/10.24072/pci.neuro.100217.rev41</a>
      </div>
    </div>`,
  '10.24072/pci.neuro.100217': `<div style="margin-top: 25px" class="pci2-recomm-review-container">
    <h4 class="pci2-recomm-text-label">Recommendation</h4>
    <div class="pci2-recomm-text">
      <div><p>An important way to reduce animal use in research is to design adequately statistically powered experiments. Critically, one needs to select an experimental task that is not too easy to solve by chance, and an appropriate number of repetitions each animal is required to perform. Optimising such parameters allows selecting the lowest number of animals required to get a robust result. While there are some estimation tools available for optimising these parameters, behavioural paradigms in neuroscience are highly diverse, and not all available tools are easily amenable for the design of all types of experiments. One such example is spatial memory paradigms such as mazes where the animal must remember the correct path to a reward (e.g. plus-maze, or radial arm maze). Desachy et al (Desachy et al., 2025) report a new freely available online tool for this type of cognitive tasks.&nbsp;</p>

<p>The authors provide a description of the statistical analyses done to design the SuccessRatePower tool, where the modification of three main parameters in the statistical design (i.e., number of trials, use of lower chance level or use of test suited for proportion comparison) allows for the increment of statistical power without the need of increasing sample size. This way, by running simulations multiple times, with different sample sizes, users can reach the experimental design that will have a specific statistical power.&nbsp;</p>

<p>The authors also include different analysis approaches, including defining the unit of measurement as either all trials across a cohort, or average performance of each animal. In addition, they include a choice of different statistical testing approaches, including summary statistics (t-test) and multilevel modelling. These are highly useful illustrations as each approach has been used in the literature, and has advantages and limitations in hierarchically clustered datasets where multiple measurements are from one animal (see, (Galbraith et al., 2010; McNabb &amp; Murayama, 2021; Bloom et al., 2022; Eleftheriou et al., 2025)).&nbsp;</p>

<p>We also wish to highlight the journey of peer-review with this specific article. First of all, we praise the dedication and time invested by all reviewers, and in particular statistics expert Daniël Lakens. The development of this preprint throughout the process is a testimony of the usefulness of peer-review. The dialogue, exchange of knowledge and acknowledgement of other colleagues’ work, all focused on achieving the common goal of ensuring the conclusions of the manuscript are consistent with its results and methodology, is the essence of what we are working for in this community initiative. At the same time, the process also evidenced to us a need for discussion and common ground between expert statisticians and wet-lab neuroscientists, which could be achieved by better training in statistical testing (Lakens, 2021; Alger, 2022).&nbsp;</p>

<p>While there may be an element of ‘it’s a matter of taste’ involved in selecting the statistical test, it is important to consider different approaches as this helps develop an intuition of the data-generating process (Bloom et al., 2022), and to avoid pitfalls, such as pseudoreplication, which has become increasingly prevalent despite increasingly rigorous statistical reporting guidelines (Eleftheriou et al., 2025).</p>

<p style="display: none;">&nbsp;</p>

<p><strong>References</strong></p>

<ol>
<li>Theo Desachy, Marc Thevenet, Samuel Garcia, Anistasha Lightning, Anne Didier, Nathalie Mandairon, Nicola Kuczewski (2025) Enhancing Statistical Power While Maintaining Small Sample Sizes in Behavioral Neuroscience Experiments Evaluating Success Rates. bioRxiv, ver.6 peer-reviewed and recommended by PCI Neuroscience <a href="https://doi.org/10.1101/2024.07.25.605060">https://doi.org/10.1101/2024.07.25.605060</a></li>
<li>Alger BE (2022) Neuroscience Needs to Test Both Statistical and Scientific Hypotheses. The Journal of Neuroscience, 42, 8432–8438. https://doi.org/10.1523/JNEUROSCI.1134-22.2022&nbsp;</li>
<li>Bloom PA, Thieu MKN, Bolger N (2022) Commentary on Unnecessary reliance on multilevel modelling to analyse nested data in neuroscience: When a traditional summary-statistics approach suffices. Current Research in Neurobiology, 3, 100041. https://doi.org/10.1016/j.crneur.2022.100041&nbsp;</li>
<li>Desachy T, Thevenet M, Garcia S, Lightning A, Didier A, Mandairon N, Kuczewski N (2025) Enhancing Statistical Power While Maintaining Small Sample Sizes in Behavioral Neuroscience Experiments Evaluating Success Rates. , 2024.07.25.605060. https://doi.org/10.1101/2024.07.25.605060&nbsp;</li>
<li>Eleftheriou C, Giachetti S, Hickson R, Kamnioti-Dumont L, Templaar R, Aaltonen A, Tsoukala E, Kim N, Fryer-Petridis L, Henley C, Erdem C, Wilson E, Maio B, Ye J, Pierce JC, Mazur K, Landa-Navarro L, Petrović NG, Bendova S, Woods H, Rizzi M, Salazar-Sanchez V, Anstey N, Asiminas A, Basu S, Booker SA, Harris A, Heyes S, Jackson A, Crocker-Buque A, McMahon AC, Till SM, Wijetunge LS, Wyllie DJ, Abbott CM, O’Leary T, Kind PC (2025) Better statistical reporting does not lead to statistical rigour: lessons from two decades of pseudoreplication in mouse-model studies of neurological disorders. Molecular Autism, 16, 30. https://doi.org/10.1186/s13229-025-00663-3&nbsp;</li>
<li>Galbraith S, Daniel JA, Vissel B (2010) A Study of Clustered Data and Approaches to Its Analysis. Journal of Neuroscience, 30, 10601–10608. https://doi.org/10.1523/JNEUROSCI.0362-10.2010&nbsp;</li>
<li>Lakens D (2021) The Practical Alternative to the p Value Is the Correctly Used p Value. Perspectives on Psychological Science, 16, 639–648. https://doi.org/10.1177/1745691620958012&nbsp;</li>
<li>McNabb CB, Murayama K (2021) Unnecessary reliance on multilevel modelling to analyse nested data in neuroscience: When a traditional summary-statistics approach suffices. Current Research in Neurobiology, 2, 100024. https://doi.org/10.1016/j.crneur.2021.100024&nbsp;</li>
</ol>

<p style="display: none;">&nbsp;</p>

<p style="display: none;">&nbsp;</p>
</div> 

      
        <div><a class="btn btn-info pci-public" href="https://neuro.peercommunityin.org/articles/rec?asPDF=True&amp;id=217"><span>PDF recommendation <img alt="pdf" src="/PCINeuro/static/_25.10.1/images/application-pdf.png"></span></a></div>
      

      
    </div>
  </div>`,
};
