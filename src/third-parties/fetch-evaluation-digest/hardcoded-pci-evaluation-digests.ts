export const hardcodedPciEvaluationDigests = {
  '10.24072/pci.neuro.100217.d1': `<div class="pci2-recomm-review-container" id="d-320">
      <h4 class="pci2-recomm-text-label">Decision by <span><span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=631"><b>Amanda Almacellas Barbanoj</b></a><a href="https://orcid.org/0000-0001-8816-2642" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0001-8816-2642"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span> and <span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=9"><b>Mahesh Karnani</b></a><a href="https://orcid.org/0000-0002-4810-4146" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0002-4810-4146"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span></span>, <span><i>posted 15 Oct 2024</i></span><span><i>, validated 15 Oct 2024</i></span></h4>
      <div class="pci2-recomm-text">
        <p>The authors present an open source statistical power calculator in binary choice tasks often used in behavioural assessment, and demonstrate its utility with an example behavioural experiment.&nbsp;</p>

<p>This preprint has been reviewed by four reviewers, who generally saw merit in the work, although some major concerns were raised by Dr Lakens and anonymous reviewer 1, that we think should be addressed in detail. Particularly, the nested structure of the data. Addressing this would be a useful service to the field, as results are sometimes tested with the assumption that repeated measurements from the same animal are independent. In addition to Dr Lakens’ references, another useful reference is <a href="https://doi.org/10.1038/nn.3648">https://doi.org/10.1038/nn.3648</a> . We feel that ideally the authors would modify their simulation code to take this into account, and perhaps make it available as an option in the GUI, such that future users can see for themselves the difference. Additionally, a figure comparing results with and without this option might be useful.&nbsp;</p>

<p><br>In addition to the reviewer’s comments, the recommenders noted additional minor points:</p>

<p>In order to understand how different the 2-arm and 4-arm situations were, the behavioural experiment would benefit from a more detailed description:</p>

<ul>
<li>Were groups 1 and 2 the same 6 mice? If not, please clarify in text under ‘Animals’ because it implies that this was the case. If yes, which was conducted first?&nbsp;</li>
<li>Please clarify definitions of session and block - if they are the same, please only use one word.</li>
<li>Was limonene the only odour?</li>
<li>What is classified as an animal visiting an arm? (reaching the reward hole, X% of the body entering the arm, or other criterion?)</li>
<li>How deep are the reward holes?</li>
<li>How big is the reward?</li>
<li>How were animals returned to the start box?</li>
<li>How were the reward holes reset? Were they cleaned? How was randomization of the correct arm sequence determined by the investigator?</li>
<li>Authors mention the NC3Rs, which also offer power analysis tools for optimising experimental design e.g., at https://eda.nc3rs.org.uk/. It may be worth noting how the authors’ tool compares to this.</li>
<li>Figures need attention:<br>Fig2, chance ½ and ¼ wrong way around, panel labels in very small font<br>Fig3, x-ticks not aligned with labels or data, spelling of ‘success’, panel labels very small.<br>Fig4, too many font sizes, clarify ‘8 trials’ refers only to block 16 e.g. with an arrow.&nbsp;</li>
</ul>


      
      </div>
    </div>`,
  '10.24072/pci.neuro.100217.rev11': `<div id="review-513" class="review-container">
      <h4 class="pci2-round-review-sub-title"><span id="review-513">Reviewed by <a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=783"><b>Georgie Mills</b></a>, 03 Oct 2024</span></h4>

      <div style="border-left: 1px solid #eee; padding: 5px 15px; margin-bottom: 25px;">
        
        <p>This preprint aims to address the specialised aspects of behavioural study designs that frequently lead to undermining reproducibility of results due to low statistical power. The authors aim to address an ongoing issue with behavioural studies and provide appropriate context to the issue being addressed.&nbsp;</p><p>The title of the paper clearly reflects the contents throughout. Hypotheses are well presented and referred back to throughout the results and discussion. The article is well constructed, providing a detailed overview of the statistical package and applying experimental data from a commonly used behaviour to test the ability of the package to do as they deem required. The authors clearly present the rationale for designing this package and motivation as to why this is required in the field of behavioural neuroscience.The methodology is properly employed and well explained. The conclusions are well supported by the results given.</p><p>The authors provide a detailed comparison of current methods of statistical testing and their designed package coupled with an excellent description of how the different methods are utilised to bridge the gap the article is addressing. This is clear, concise but detailed in an understandable way. This enables users to easily utilise this within their study design in any lab.&nbsp;</p><p>The results clearly show how the statistical package has been generated that can enable small sample sizes to gain high statistical power. Mainly through the use of a monte-carlo simulation. Application of this method on experimental data concluded that reducing chance level enabled significance to be reached, at a level where 5xs more animals would normally be required. Highlighting the advantage of using this statistical package from an ethical viewpoint, thereby addressing the question if using less animals can impact the quality outcomes of statistics performed on the test. This is an aspect I congratulate the authors on highlighting and aiming to overcome. The consideration of different parameters that can affect significance is a strong point of this preprint. And importantly, bringing to question how experimental design can impact the outcomes of a study in a niche where these issues are prevalent.&nbsp;</p><p>The queries I had with the approach and design are well discussed within the limitations of the study, making this well-balanced in the discussion. For example, the issue of using the same number of trials for each animal tested within the study. An aspect which could cause performance differences in each subject due to testing some animals more than others. I would also query how much should be changed in terms of experimental design to gain statistical significance, I would like to hear from the authors in the discussion on what is the limit to manipulate the data in terms of experimental design using this statistical package.&nbsp;</p><p>Minor corrections: For figure 2, labels are opposing each other from in-text and in-figure details. They currently show 2 arms with chance ¼ and group 4 arms of chance ½ - this is opposite as to what is in the text.</p><p>I would be interested to see how this methodology impacts other behavioural paradigms widely used in neuroscience where fatigue plays a large factor when increasing trial number. Or, a reward-independent paradigm, where sometimes variability is higher between mice. As currently, I would infer this method is only successful for associative learning paradigms. Hence I think comparison to another behaviour would support this notion to a higher degree.&nbsp;</p><p>It would also be interesting to understand within the preprint how varying the monte-carlo simulation number of iterations would impact the outcome due to noting “a higher number provides greater precision in the power estimation.”&nbsp;</p><p>But overall, a really nice approach to bridging the gap in low sample sizes and the issue of statistical significance in behavioural neuroscience where variability in cognitive ability is present. A great benefit to current aspects of behavioural studies.&nbsp;I congratulate the authors in providing a means to optimise the experimental design for behavioural paradigms for associative learning&nbsp;</p><p style="display: none;">&nbsp;</p><p style="display: none;">&nbsp;</p>

        

        
        <a class="doi-url-subsection" href="https://doi.org/10.24072/pci.neuro.100217.rev11" target="_blank">https://doi.org/10.24072/pci.neuro.100217.rev11</a>
      </div>
    </div>`,
};
