export const hardcodedPciEvaluationDigests = {
  '10.24072/pci.neuro.100217.d1': `<div class="pci2-recomm-review-container" id="d-320">
      <h4 class="pci2-recomm-text-label">Decision by <span><span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=631"><b>Amanda Almacellas Barbanoj</b></a><a href="https://orcid.org/0000-0001-8816-2642" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0001-8816-2642"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span> and <span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=9"><b>Mahesh Karnani</b></a><a href="https://orcid.org/0000-0002-4810-4146" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0002-4810-4146"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span></span>, <span><i>posted 15 Oct 2024</i></span><span><i>, validated 15 Oct 2024</i></span></h4>
      <div class="pci2-recomm-text">
        <p>The authors present an open source statistical power calculator in binary choice tasks often used in behavioural assessment, and demonstrate its utility with an example behavioural experiment.&nbsp;</p>

<p>This preprint has been reviewed by four reviewers, who generally saw merit in the work, although some major concerns were raised by Dr Lakens and anonymous reviewer 1, that we think should be addressed in detail. Particularly, the nested structure of the data. Addressing this would be a useful service to the field, as results are sometimes tested with the assumption that repeated measurements from the same animal are independent. In addition to Dr Lakens’ references, another useful reference is <a href="https://doi.org/10.1038/nn.3648">https://doi.org/10.1038/nn.3648</a> . We feel that ideally the authors would modify their simulation code to take this into account, and perhaps make it available as an option in the GUI, such that future users can see for themselves the difference. Additionally, a figure comparing results with and without this option might be useful.&nbsp;</p>

<p><br>In addition to the reviewer’s comments, the recommenders noted additional minor points:</p>

<p>In order to understand how different the 2-arm and 4-arm situations were, the behavioural experiment would benefit from a more detailed description:</p>

<ul>
<li>Were groups 1 and 2 the same 6 mice? If not, please clarify in text under ‘Animals’ because it implies that this was the case. If yes, which was conducted first?&nbsp;</li>
<li>Please clarify definitions of session and block - if they are the same, please only use one word.</li>
<li>Was limonene the only odour?</li>
<li>What is classified as an animal visiting an arm? (reaching the reward hole, X% of the body entering the arm, or other criterion?)</li>
<li>How deep are the reward holes?</li>
<li>How big is the reward?</li>
<li>How were animals returned to the start box?</li>
<li>How were the reward holes reset? Were they cleaned? How was randomization of the correct arm sequence determined by the investigator?</li>
<li>Authors mention the NC3Rs, which also offer power analysis tools for optimising experimental design e.g., at https://eda.nc3rs.org.uk/. It may be worth noting how the authors’ tool compares to this.</li>
<li>Figures need attention:<br>Fig2, chance ½ and ¼ wrong way around, panel labels in very small font<br>Fig3, x-ticks not aligned with labels or data, spelling of ‘success’, panel labels very small.<br>Fig4, too many font sizes, clarify ‘8 trials’ refers only to block 16 e.g. with an arrow.&nbsp;</li>
</ul>


      
      </div>
    </div>`,
  '10.24072/pci.neuro.100217.rev11': `<div id="review-513" class="review-container">
      <h4 class="pci2-round-review-sub-title"><span id="review-513">Reviewed by <a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=783"><b>Georgie Mills</b></a>, 03 Oct 2024</span></h4>

      <div style="border-left: 1px solid #eee; padding: 5px 15px; margin-bottom: 25px;">
        
        <p>This preprint aims to address the specialised aspects of behavioural study designs that frequently lead to undermining reproducibility of results due to low statistical power. The authors aim to address an ongoing issue with behavioural studies and provide appropriate context to the issue being addressed.&nbsp;</p><p>The title of the paper clearly reflects the contents throughout. Hypotheses are well presented and referred back to throughout the results and discussion. The article is well constructed, providing a detailed overview of the statistical package and applying experimental data from a commonly used behaviour to test the ability of the package to do as they deem required. The authors clearly present the rationale for designing this package and motivation as to why this is required in the field of behavioural neuroscience.The methodology is properly employed and well explained. The conclusions are well supported by the results given.</p><p>The authors provide a detailed comparison of current methods of statistical testing and their designed package coupled with an excellent description of how the different methods are utilised to bridge the gap the article is addressing. This is clear, concise but detailed in an understandable way. This enables users to easily utilise this within their study design in any lab.&nbsp;</p><p>The results clearly show how the statistical package has been generated that can enable small sample sizes to gain high statistical power. Mainly through the use of a monte-carlo simulation. Application of this method on experimental data concluded that reducing chance level enabled significance to be reached, at a level where 5xs more animals would normally be required. Highlighting the advantage of using this statistical package from an ethical viewpoint, thereby addressing the question if using less animals can impact the quality outcomes of statistics performed on the test. This is an aspect I congratulate the authors on highlighting and aiming to overcome. The consideration of different parameters that can affect significance is a strong point of this preprint. And importantly, bringing to question how experimental design can impact the outcomes of a study in a niche where these issues are prevalent.&nbsp;</p><p>The queries I had with the approach and design are well discussed within the limitations of the study, making this well-balanced in the discussion. For example, the issue of using the same number of trials for each animal tested within the study. An aspect which could cause performance differences in each subject due to testing some animals more than others. I would also query how much should be changed in terms of experimental design to gain statistical significance, I would like to hear from the authors in the discussion on what is the limit to manipulate the data in terms of experimental design using this statistical package.&nbsp;</p><p>Minor corrections: For figure 2, labels are opposing each other from in-text and in-figure details. They currently show 2 arms with chance ¼ and group 4 arms of chance ½ - this is opposite as to what is in the text.</p><p>I would be interested to see how this methodology impacts other behavioural paradigms widely used in neuroscience where fatigue plays a large factor when increasing trial number. Or, a reward-independent paradigm, where sometimes variability is higher between mice. As currently, I would infer this method is only successful for associative learning paradigms. Hence I think comparison to another behaviour would support this notion to a higher degree.&nbsp;</p><p>It would also be interesting to understand within the preprint how varying the monte-carlo simulation number of iterations would impact the outcome due to noting “a higher number provides greater precision in the power estimation.”&nbsp;</p><p>But overall, a really nice approach to bridging the gap in low sample sizes and the issue of statistical significance in behavioural neuroscience where variability in cognitive ability is present. A great benefit to current aspects of behavioural studies.&nbsp;I congratulate the authors in providing a means to optimise the experimental design for behavioural paradigms for associative learning&nbsp;</p><p style="display: none;">&nbsp;</p><p style="display: none;">&nbsp;</p>

        

        
        <a class="doi-url-subsection" href="https://doi.org/10.24072/pci.neuro.100217.rev11" target="_blank">https://doi.org/10.24072/pci.neuro.100217.rev11</a>
      </div>
    </div>`,
  '10.24072/pci.neuro.100217.rev12': `<div id="review-516" class="review-container">
      <h4 class="pci2-round-review-sub-title"><span id="review-516">Reviewed by <span><a class="cyp-user-profile-link" href="https://neuro.peercommunityin.org/public/user_public_page?userId=786"><b>Daniel Lakens</b></a><a href="https://orcid.org/0000-0002-0247-239X" rel="noreferrer noopener" style="margin-right: 3px; position: relative; bottom: 12px; left: 2px" target="_blank" title="0000-0002-0247-239X"><img alt="ORCID_LOGO" heigth="12px" src="/PCINeuro/static/_25.10.1/images/ORCID_ID.svg" width="12px"></a></span>, 08 Oct 2024</span></h4>

      <div style="border-left: 1px solid #eee; padding: 5px 15px; margin-bottom: 25px;">
        
        <p>This paper is written around the code for a power simulation for a single specific study. It is relatively uncommon to publish code for a specific power simulation as a separate article. A problem is that the power analysis as it is simulated is fundamentally flawed, as the authors ignore the multi-level (or hierarchical) structure in the data, and treat the repeated trials from the same animals as independent data (as far as I could tell from the code). This is not correct.&nbsp;</p><p style="display: none;">&nbsp;</p><p>1. The authors should simulate a multi-level test (i.e., multi-level logistic regression)</p><p style="display: none;">&nbsp;</p><p>The reason for this is that the moment the same individual performs multiple trials, we have a multi-level structure – trials are nested inside mice, and mice are nested in condition. We can not treat all trials from the mice as independent from each other – they are not all independent observations – some come from the same mice.&nbsp;</p><p style="display: none;">&nbsp;</p><p>The basic idea of such power analyses is nice explained in: Westfall, J., Kenny, D. A., &amp; Judd, C. M. (2014). Statistical power and optimal design in experiments in which samples of participants respond to samples of stimuli. Journal of Experimental Psychology: General, 143(5), 2020.</p><p style="display: none;">&nbsp;</p><p>The authors are performing power analyses for a multilevel logistic regression, if I understood their goals well. This is explained in the following paper:&nbsp;</p><p>Olvera Astivia, O. L., Gadermann, A., &amp; Guhn, M. (2019). The relationship between statistical power and predictor distribution in multilevel logistic regression: A simulation-based approach. BMC Medical Research Methodology, 19(1), 97. https://doi.org/10.1186/s12874-019-0742-8</p><p>There is also an associated shiny app to perform these calculations.&nbsp;</p><p>It was difficult to clearly see what the authors intended to do in their power analysis. There is no math, not clear discussion of the tests they are simulating. If I look at their code, line 79 says:&nbsp;</p><p style="display: none;">&nbsp;</p><p>p=scipy.stats.ttest_ind(animals_sucess1,animals_sucess2,alternative=alter)[1]”</p><p style="display: none;">&nbsp;</p><p>I am not familiar with statistical tests in python (I use R) but it seems the authors are just running a t-test. This is clearly wrong. It would probably be a good idea to invite a statistician to collaborate on this project.&nbsp;</p><p style="display: none;">&nbsp;</p><p>I might have gotten this wrong – if so, my apologies, but then my review at least explains that it is not clear to me which power analysis is performed. The authors should provide much more detail.&nbsp;</p><p style="display: none;">&nbsp;</p><p>2. There are no validation documents</p><p style="display: none;">&nbsp;</p><p>Typically when authors want to publish software for power analysis, I would expect to see extensive validation documents. This consists of an overview of simulations that show expected results - for example a nominal Type 1 error rate, and the same results as other power analysis tools.&nbsp;</p><p style="display: none;">&nbsp;</p><p>For example, when there are no effects, we should observe nominal alpha levels. So, if a 5% alpha is used, 5% of the tests are significant.&nbsp;</p><p style="display: none;">&nbsp;</p><p>When I ran the software with no differences between groups, and equal groups, 1 trial, 1 repetition, and 100.000 trials the Type 1 error rate was 6.14. It always was a bit high when I tested it. Of course, it should be 5%. If it is too high, this might be a problem with the code. It is difficult to say, but I personally would not trust the code.&nbsp;</p><p style="display: none;">&nbsp;</p><p>If you search the literature, I believe there is other software out there that can perform at least some of the same tests. The validation document should then demonstrate that the same results are observed. In this case, as the authors do not actually simulate a multi-level data structure and perform a multi-level test, so their results will not align, but this would have provided them an indication that they are doing something wrong. Performing these simulations is difficult, as you need the within and between cluster correlations, which are very difficult to know (or to estimate with the small sample sizes used in animal research). Nevertheless, these are essential for valid simulations.</p><p style="display: none;">&nbsp;</p><p>3. Discussion of a real study</p><p style="display: none;">&nbsp;</p><p>The article also presents information about the single mouse study the analysis was created for, but all empirical sections are superfluous. There is no added value in discussing this study, and the methods – it is completely irrelevant for the power analysis tool. These sections should not be part of a paper on power analysis.&nbsp;</p><p>Although I think the empirical sections should be removed, there was no link in the sentence: "Both the behavioral protocol and analytical plan were pre-registered (here, OSF)."</p><p style="display: none;">&nbsp;</p><p>4. Minor:&nbsp;</p><p>When I run the code, a warning appearer that worry me. Maybe they should not worry me, but, perhaps, these are problems – regardless, it is not nice to let users worry about warnings. Either prevent them, or explain why they occur and why I do not need to worry. I got another warning one time but I did not store it. Perhaps test the code a bit more.&nbsp;</p><p>/srv/conda/envs/notebook/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.</p><p>&nbsp; return _methods._mean(a, axis=axis, dtype=dtype,</p><p>/srv/conda/envs/notebook/lib/python3.12/site-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide</p><p>&nbsp; ret = ret.dtype.type(ret / rcount)</p><p style="display: none;">&nbsp;</p><p>I saw the following spelling error in the tool : ### **SuccesRatePower**: statistical power calculator for bheavioural neuroscience</p><p style="display: none;">&nbsp;</p><p>There was also a spelling error: oeverlook</p><p>The authors write: "These results highlight a significant advantage in reducing the number of subjects required by adopting an experimental procedure with a low chance level." Of course if we use the largest manipulation we can think of, the effect size is large, and therefore power is higher, all else equal. This is a trivial fact mentioned in many power analysis papers. But, typically we do not choose an effect size to study - we choose a research question, which has a certain effect size. So, we should not choose large effects to study, but study interesting questions.&nbsp;</p><p style="display: none;">&nbsp;</p><p>The authors use Bayes factors to support the null, but do not discuss their priors. This is extremely bad statistical practice. Either use meaningful priors, or use equivalence testing instead for null effects.&nbsp;</p><p style="display: none;">&nbsp;</p><p>Signed,</p><p style="display: none;">&nbsp;</p><p>Daniel Lakens</p>

        

        
        <a class="doi-url-subsection" href="https://doi.org/10.24072/pci.neuro.100217.rev12" target="_blank">https://doi.org/10.24072/pci.neuro.100217.rev12</a>
      </div>
    </div>`,
};
